{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV4njfvTJ9bH",
        "outputId": "04729c65-3819-42de-a5a4-30246f581495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "oO7qXr73J9Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en_core_web_sm')   # 영어 토큰화\n",
        "spacy_de = spacy.load('de_core_news_sm')  # 독일어 토큰화"
      ],
      "metadata": {
        "id": "BcPoGkHeJ9W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4GMoZYrJ9VF",
        "outputId": "553734b5-c24b-4cde-ad50-fdef96e03e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 독일어(Deutsch) 문장을 토큰화 하는 함수 (순서를 뒤집지 않음)\n",
        "def tokenize_de(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "# 영어(English) 문장을 토큰화 하는 함수\n",
        "def tokenize_en(text):\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "Uhj6y1HVJ9Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "# Field 라이브러리 활용해서 데이터셋에 대한 구체적인 전처리 내용 명시\n",
        "SRC = Field(tokenize=tokenize_de, init_token=\"\", eos_token=\"\", lower=True, batch_first=True) # 독일어\n",
        "TRG = Field(tokenize=tokenize_en, init_token=\"\", eos_token=\"\", lower=True, batch_first=True) # 영어"
      ],
      "metadata": {
        "id": "n7w1KAlVKicY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대표 영어-독어 번역 데이터셋 불러오기\n",
        "from torchtext.datasets import Multi30k\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"
      ],
      "metadata": {
        "id": "Y_AJF-lwKnVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1ee753-3d73-4b76-c508-7eb0a5d14fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 945kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 235kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 224kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n",
        "print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n",
        "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yG0q0EgKnTJ",
        "outputId": "740270aa-b7c4-4a05-a869-c71db8540c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터셋(training dataset) 크기: 29000개\n",
            "평가 데이터셋(validation dataset) 크기: 1014개\n",
            "테스트 데이터셋(testing dataset) 크기: 1000개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 중 하나를 선택해 출력\n",
        "print(vars(train_dataset.examples[30])['src'])\n",
        "print(vars(train_dataset.examples[30])['trg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktx0E7TbKnRX",
        "outputId": "1b0e6881-b12a-4b0a-af31-fa4802860ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n",
            "['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# field 객체의 build_vocab 메서를 이용해 영어와 독어 단어사전 생성 (최소 2번 이상 등장한 단어만 사용)\n",
        "SRC.build_vocab(train_dataset, min_freq=2)\n",
        "TRG.build_vocab(train_dataset, min_freq=2)\n",
        "\n",
        "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
        "print(f\"len(TRG): {len(TRG.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX8KTiWbKnPM",
        "outputId": "b7e05fab-252f-4d92-ed4e-2ca788fd355d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(SRC): 7852\n",
            "len(TRG): 5892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
        "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
        "print(TRG.vocab.stoi[\"\"]) # : 2\n",
        "print(TRG.vocab.stoi[\"\"]) # : 3\n",
        "print(TRG.vocab.stoi[\"hello\"])\n",
        "print(TRG.vocab.stoi[\"world\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP8n85MfKnNM",
        "outputId": "16d7689d-4cbf-4f8a-c81f-35750bd9687f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "2\n",
            "4111\n",
            "1751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 문장에 포함된 단어가 순서대로 나열된 상태로 네트워크에 입력되어야\n",
        "## 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋음\n",
        "## 이를 위해 BucketIterator를 사용\n",
        "## 배치 크기(batch size): 128\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# 일반적인 데이터 로더(data loader)의 iterator와 유사하게 사용 가능\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "p_Zap2mBLHlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTMheoHLTRS",
        "outputId": "b7b88990-6993-4ba2-befd-9ea88760dff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[1]):\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMofeDYjLUfm",
        "outputId": "504c5e55-06f8-452f-c08a-1a41f121ff4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치 크기: torch.Size([128, 29])\n",
            "인덱스 0: 2\n",
            "인덱스 1: 4\n",
            "인덱스 2: 567\n",
            "인덱스 3: 10\n",
            "인덱스 4: 356\n",
            "인덱스 5: 1813\n",
            "인덱스 6: 20\n",
            "인덱스 7: 76\n",
            "인덱스 8: 1820\n",
            "인덱스 9: 8\n",
            "인덱스 10: 2769\n",
            "인덱스 11: 74\n",
            "인덱스 12: 6\n",
            "인덱스 13: 5\n",
            "인덱스 14: 802\n",
            "인덱스 15: 3\n",
            "인덱스 16: 2\n",
            "인덱스 17: 1\n",
            "인덱스 18: 1\n",
            "인덱스 19: 1\n",
            "인덱스 20: 1\n",
            "인덱스 21: 1\n",
            "인덱스 22: 1\n",
            "인덱스 23: 1\n",
            "인덱스 24: 1\n",
            "인덱스 25: 1\n",
            "인덱스 26: 1\n",
            "인덱스 27: 1\n",
            "인덱스 28: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_W_3Wi0J1Y_"
      },
      "source": [
        "## Packages and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELUgtAaRJ1ZC"
      },
      "outputs": [],
      "source": [
        "# 필요한 패키지 불러오기\n",
        "import torch\n",
        "from torch import nn # neural network base class\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyRGuttSJ1ZE"
      },
      "outputs": [],
      "source": [
        "# GPU 가속 확인\n",
        "# print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
        "# print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True여야 함\n",
        "# print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True여야 함\n",
        "# # !python -c 'import platform;print(platform.platform())'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH1BbVaqJ1ZE"
      },
      "outputs": [],
      "source": [
        "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEJu4ZyDJ1ZF"
      },
      "source": [
        "## 1. Embedding\n",
        "token embedding(=input embedding)과 positional embedding으로 이루어져 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrYslutaJ1ZF"
      },
      "source": [
        "### (1) Token(=Input) Embedding\n",
        "- dense representation of tokens\n",
        "- input에 입력된 데이터를 행렬로 변환하는 과정 (=임베딩)\n",
        "    - 트랜스포머 논문에서 각 토큰은 512차원으로 임베딩 됨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6JQt_yPJ1ZF"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        '''\n",
        "        vocab_size: size of vocab\n",
        "        d_model: dimension of the model\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # embedding에도 scaling 적용\n",
        "        ## positional embedding값이 더해지면서 임베딩 벡터의 값이 희석되는 것을 막기 위함\n",
        "        out = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gldq63JJ1ZG"
      },
      "source": [
        "### (2) Positional Embedding\n",
        "- inject info about the position of the tokens in sequence   \n",
        "=> compute sinusoid encoding\n",
        "- RNN 계열의 모델은 입력된 문장을 순차적으로 처리하는 반면, 트랜스포머는 입력된 문장을 병렬로 한 번에 처리함   \n",
        "=> 병렬로 처리함으로써 연산은 빨라지지만 토큰의 위치 혹은 순서를 알 수 없게 되어 위치 정보를 별도로 넣어줘야 함\n",
        "\n",
        "[주의점]\n",
        "1. 모든 위치 값은 sequence의 길이에 상관없이 동일한 식별자를 가져야 함 (sequence 내용이 바뀌어도 임베딩 값이 같아야 함)\n",
        "2. 모든 위치 값은 너무 크면 안됨 (단어 의미 정보 변질: 단어 간 상관관계나 의미를 유추할 수 있는 정보의 중요도가 상대적으로 작아지게 되어 제대로 된 학습이 되지 않을 수 있음)   \n",
        "=> sequence 크기에 비례하여 일정하게 커지는 정수값 부여, normalization 적용 등은 이러한 이유 때문에 사용 불가능   \n",
        "=> sinusoid encoding 활용!!\n",
        "    - sine, cosine은 -1과 1을 사이를 반복하는 주기함수 (값이 너무 커지지 않음)\n",
        "    - 그러나 위치 벡터 값이 같아지는 문제 발생할 수 있음 (예를 들어, 포지션 0과 포지션 n에서 sine 값이 같다면)\n",
        "        => 따라서 sine과 cosine 동시에 사용\n",
        "\n",
        "> PE_(pos, 2i) = sin(pos/10000^{2i/d_model})\n",
        "> PE_(pos, 2i+1) = cos(pos/10000^{2i/d_model})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAw9g7ydJ1ZG"
      },
      "source": [
        "#### Positional Encoding 시각화\n",
        "https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CamdTxcWJ1ZH"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return pos_encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "GjhguJ-7J1ZH",
        "outputId": "d78194a7-d311-4089-fd79-5e115dfba574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 512)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokens = 10         # 10개 단어\n",
        "dimensions = 512    # 512 차원\n",
        "\n",
        "pos_encoding = positional_encoding(tokens, dimensions)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.pcolormesh(pos_encoding[0], cmap='viridis')\n",
        "plt.xlabel('Embedding Dimensions')\n",
        "plt.xlim((0, dimensions))\n",
        "plt.ylim((tokens,0))\n",
        "plt.ylabel('Token Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SQOat1J1ZI"
      },
      "source": [
        "#### Positional Embedding 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9j_xjhlJ1ZI"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, d_model, device):\n",
        "        '''\n",
        "        max_len: max length of sequence\n",
        "        d_model: dimension of the model\n",
        "        device: setting\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # 비어 있는 텐서 생성\n",
        "        self.PE = torch.zeros(max_len, d_model, device=device)\n",
        "        # no need to compute gradient (학습되는 값이 아님, 공식에 의해 계산됨)\n",
        "        self.PE.requires_grad = False\n",
        "\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(dim=1)\n",
        "        for_2i = torch.arange(0, d_model, step=2, dtype=torch.float, device=device)\n",
        "\n",
        "        # 문장 길이가 달라져도 단어 사이 거리를 보존할 수 있는 sinusoid 사용\n",
        "        ## PE_(pos,2i)는 단어 pos의 position embedding vector의 위치 2i의 원소값을 의미\n",
        "        self.PE[:, 0::2] = torch.sin(pos / (10000 ** (for_2i / d_model))) # 2i가 짝수인 경우\n",
        "        self.PE[:, 1::2] = torch.cos(pos / (10000 ** (for_2i / d_model))) # 2i가 홀수인 경우\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_len = x.size()\n",
        "\n",
        "        return self.PE[:seq_len, :] # input의 length에 맞춰서 positional encoding 결과 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-25HCRwJ1ZI"
      },
      "outputs": [],
      "source": [
        "# torch.arange(0, 3).unsqueeze(dim=1)\n",
        "# torch.arange(0, 6).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTILTpQZJ1ZI"
      },
      "source": [
        "### (3) Embedding (token+positional)\n",
        "- token embedding과 positional embedding을 summation해서 사용\n",
        "- 왜 concat하지 않았을까?  \n",
        "\n",
        "[concatenation]\n",
        "- token 내용은 자체 차원 공간을 갖게 되고, positional 정보 또한 자체 차원 공간을 갖게 되어 둘은 서로 관계 없는 공간에 위치하게 됨\n",
        "- 이점: 정보가 뒤섞이지 않음\n",
        "- 문제: 메모리, 파라미터, 런타임 등에 관련된 비용 문제 발생\n",
        "\n",
        "[summation]\n",
        "- 이점: token 내용과 position 정보의 균형을 잘 맞출 수 있음 (벡터 공간에서 단어 의미 정보와 위치 정보 간의 거리 적절하게 유지)\n",
        "- 문제: 정보 뒤섞일 수 있음\n",
        "\n",
        "=> GPU 등 하드웨어 성능 좋으면 concat 해서 사용해도 괜찮을지도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBtcxL7aJ1ZI"
      },
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, max_len, device):\n",
        "        super().__init__()\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEmbedding(max_len, d_model, device)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(x)\n",
        "        out = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Kiff9NJ1ZI"
      },
      "source": [
        "## 2. Sub-layers\n",
        "multi-head attention   \n",
        "    -  scaled dot-product attention   \n",
        "position-wise fully connected feed-forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip-tjU70J1ZI"
      },
      "source": [
        "<center><img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" height=\"300\" width=\"300\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Q3LQIwJ1ZJ"
      },
      "source": [
        "### (1) Scaled Dot-Product Attention\n",
        "> Attention(Q, K, V) = softmax(QK^T/(d_k)**0.5)V\n",
        "\n",
        "[순서]\n",
        "1. Transpose K   \n",
        "2. MatMul of Q and K^T => attention score   \n",
        "3. scale (for stable gradients)   \n",
        "4. (mask to -inf if necessary)   \n",
        "5. apply softmax => 가중치 도출 (determines how much each word will be expressed at this position)    \n",
        "6. MatMul of attention result and V (the intuition here is to keep intact the values of word(s) we want to focus on, and drown-out irrelevant words (by multiplying small weights))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yedt2xtIJ1ZJ"
      },
      "source": [
        "<center> <img src=\"https://raw.githubusercontent.com/angiekim05/study/master/Paper_Code_Practice/Transformer/src/scaled_dot_product_attention.png\" width=\"200\" height=\"270\"> </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tshRBkVsJ1ZJ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1) # apply softmax to the last dimension\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # 4D tensor as input: [batch_size, n_heads, seq_len, head_dim]\n",
        "        _, _, _, head_dim = k.size()\n",
        "\n",
        "        # transpose K\n",
        "        k_t = k.transpose(2, 3)\n",
        "\n",
        "        # matmul of Q and K^T + scale\n",
        "        ## Q @ K^T = [batch_size, n_heads, q_seq_len, k_seq_len]\n",
        "        score = torch.matmul(q, k_t) / math.sqrt(head_dim)\n",
        "\n",
        "        # masking (opt.)\n",
        "        ## torch.tensor.masked_fill(): 텐서의 특정 값을 다른 값으로 바꾸고자 할 때 사용\n",
        "        ## mask: 단어가 있는 곳(True), 마스킹 되어 단어가 없는 곳(False, 0)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask==0, -10000000000) # mask가 0인 경우 -inf로 채움 (softmax 계산 시 0이 되게끔)\n",
        "\n",
        "        # apply softmax (weight)\n",
        "        ## score = [batch_size, n_heads, q_seq_len, k_seq_len]\n",
        "        score = self.softmax(score)\n",
        "        # score = self.dropout(score)\n",
        "\n",
        "        # multiply with Value (weighted sum)\n",
        "        ## result = [batch_size, n_heads, seq_len, head_dim] (k, v는 seq_len이 같음)\n",
        "        result = torch.matmul(score, v)\n",
        "\n",
        "        return result, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pptXlLRKJ1ZJ"
      },
      "source": [
        "### (2) Multi-head Attention\n",
        "단일 attention을 사용할 때보다 multi-head attention을 사용했을 때 문장의 특징 정보를 더 많이 잡을 수 있음\n",
        "> Q, K, V 행렬(d_model=512차원)을 head의 수(8개)만큼 나누어서 진행(64차원)   \n",
        "> head는 scaled dot-product attention   \n",
        "\n",
        "[순서]\n",
        "1. Q, K, V 행렬을 64차원으로 projection (linear(fc) 통과)\n",
        "2. h=8로 분리\n",
        "3. scaled dot-product attention 수행 (scaled dot-product att 8개가 하나의 attention이 되는 구조)\n",
        "4. head concat (512차원)\n",
        "5. projection (성능향상 목적)   \n",
        "=> 각 단어에 대해 최종 512차원 벡터 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veuRDUMOJ1ZJ"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/angiekim05/study/master/Paper_Code_Practice/Transformer/src/multi_head_attention.png\" width=\"220\" height=\"270\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCSp2KceJ1ZJ"
      },
      "source": [
        "<center><img src=\"https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/transformer.png\" height=\"450\" width=\"750\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-rY6qqJ1ZJ"
      },
      "source": [
        "<center><img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"700\" height=\"450\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckP3TY2gJ1ZJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # 사용될 구성 정의\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model              # 512\n",
        "        self.n_heads = n_heads              # 8\n",
        "        self.head_dim = d_model // n_heads  # 64\n",
        "\n",
        "        # nn.Linear(in,out): input size와 output size 입력\n",
        "        ## 논문의 설명에서는 seq_len * head_dim의 크기로 Q, K, V를 구해서 각각 attention을 계산하여 concat하는 방식을 쓰지만\n",
        "        ## 별개의 attention 연산을 head회 수행해야 한다는 점에서 매우 비효율적\n",
        "        ## 따라서, 실제로는 seq_len * d_model로 한 번의 attention 계산으로 처리\n",
        "        ##\n",
        "        self.w_q = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
        "        self.w_o = nn.Linear(n_heads * self.head_dim, d_model, bias=False)\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    # 실행 계산 정의 (구성 연결)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # shape: [batch_size, seq_len, d_model]\n",
        "        batch_size, _, _ = query.shape\n",
        "        # apply weight matrices (=projection)\n",
        "        q, k, v = self.w_q(query), self.w_k(key), self.w_v(value)\n",
        "\n",
        "        # split into heads\n",
        "        ## shape: [batch_size, n_heads, seq_len, head_dim]으로\n",
        "        ## seq_len=-1로 설정하는 이유는 decoder 단에서는 q와 k, v의 seq_len이 다를 수 있기 때문\n",
        "        q = q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # apply scaled dot-product attention\n",
        "        att_result, attention_score = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # concat and pass to next layer (=Linear)\n",
        "        ## att_result = [batch_size, n_heads, seq_len, head_dim]\n",
        "        att_result = att_result.transpose(1, 2) # [batch_size, seq_len, n_heads, head_dim]\n",
        "        att_result = att_result.contiguous().view(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        att_result = self.w_o(att_result) # [batch_size, seq_len, d_model]\n",
        "\n",
        "        return att_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHa_nxUBJ1ZJ"
      },
      "source": [
        "### (3) Position-wise Fully Connected Feed Forward Network\n",
        "- applied to each position separately and identically (병렬 가능)\n",
        "- linear transformations + ReLU + linear transformation 구조\n",
        "- FFN(x) = max(0, xW_1 + b_1) W_2 + b_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGBCgcJjJ1ZK"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        # inner-layer dimensionality: d_ff = 2048\n",
        "        # input and output dimensionality: d_model = 512\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37nMZHnLJ1ZK"
      },
      "source": [
        "### (4) Layer Normalization\n",
        "- 각 샘플에 대해서 feature들에 대한 평균과 분산 구해서 정규화\n",
        "- = nn.LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRvoTAAKJ1ZK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super().__init__()\n",
        "        # nn.Parameter는 파라미터로 지정하고 싶은 tensor일 때 사용\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model)) # weight\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model)) # bias\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True) # keepdim: 차원유지 (broadcasting 가능하게끔)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True) # unbiased: calculate variance w/o Bessel's correction\n",
        "\n",
        "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beqKyP06J1ZK"
      },
      "source": [
        "## 3. Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhjeonJJ1ZK"
      },
      "source": [
        "### (1) Encoder Block\n",
        "<center><img src=\"https://yangoos57.github.io/static/6e332ac57f0ed3147513504101b18da5/3c492/encoder_block_0.png\" width=\"400\" height=\"400\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KpUt2zlJ1ZK"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, n_heads):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        '''\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "        padding_mask: [batch_size, 1, 1, seq_len]\n",
        "        '''\n",
        "\n",
        "        # FIRST SUBLAYER\n",
        "        _x = x\n",
        "        # q, k, v는 표현만 다를 뿐, input 동일하게 사용\n",
        "        ## mask, so that attention is not applied to pad tokens\n",
        "        x = self.attention(query=x, key=x, value=x, mask=padding_mask)\n",
        "        # \"apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized\"\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x) # x + _x: Residual Connection\n",
        "        # => output: 512 dimension\n",
        "\n",
        "        # SECOND SUBLAYER\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "        # => output: 512 dimension\n",
        "\n",
        "        return x # => output: 512 dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB_F9o-HJ1ZK"
      },
      "source": [
        "### (2) Encoder\n",
        "<center><img src=\"https://yangoos57.github.io/static/dd5c378de3e1a44105474c8c9c08dad9/3c492/encoder.png\" width=\"450\" height=\"400\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONJNdVh5J1ZK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, enc_vocab_size, max_len, d_model, d_ff, n_heads, n_layers, device):\n",
        "        super().__init__()\n",
        "        # input\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=enc_vocab_size,\n",
        "                                        device=device)\n",
        "        # 6 encoder blocks\n",
        "        ## nn.ModuleList: nn.Sequential과 유사하지만 nn.Sequential은 자동으로, 순차적으로 forward 메소드가 실행되기 때문에 한 단위로 실행되지만\n",
        "        ##                nn.ModuleList의 경우 for문을 통해 iterate 하면서 리스트 내의 모듈을 각각 수행시키는 것\n",
        "        self.layers = nn.ModuleList([EncoderBlock(d_model=d_model, d_ff=d_ff, n_heads=n_heads)\n",
        "                                    for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        '''\n",
        "        x: [batch_size, seq_len]\n",
        "        padding_mask: [batch_size, 1, 1, seq_len]\n",
        "        '''\n",
        "\n",
        "        # compute vector (sentence -> vector)\n",
        "        x = self.emb(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        return x # => 인코더의 최종 결과값은 context (디코더에서 key와 value로 활용됨)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-te_lR1J1ZK"
      },
      "source": [
        "### (3) Decoder Block\n",
        "<center><img src=\"https://yangoos57.github.io/static/5f18ca895d929c0c6041572c5e302c65/3c492/decoder_block_3.png\" width=\"500\" height=\"400\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdA9G66UJ1ZK"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.enc_dec_att = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
        "        self.norm3 = LayerNorm(d_model=d_model)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, dec, context, no_look_mask, padding_mask):\n",
        "        '''\n",
        "        dec: [batch_size, seq_len]\n",
        "        context: [batch_size, seq_len, d_model]\n",
        "        no_look_mask: [batch_size, 1, trg_len, trg_len]\n",
        "        padding_mask: [batch_size, 1, 1, src_len]\n",
        "        '''\n",
        "\n",
        "        _x = dec\n",
        "        # masked multi-head attention (mask=pad masking+subsequent masking)\n",
        "        ## to prevent positions from attending to subsequent positions\n",
        "        ## => ensures that the predictions for position i can depend only on the known outputs at positions less than i\n",
        "        x = self.self_attention(query=dec, key=dec, value=dec, mask=no_look_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        if context is not None:\n",
        "            _x = x\n",
        "            # query: decoder attention의 결과 (예: 영어), key/value: encoder 결과 (예: 한국어)\n",
        "            x = self.enc_dec_att(query=x, key=context, value=context, mask=padding_mask)\n",
        "            x = self.dropout2(x)\n",
        "            x = self.norm2(x + _x)\n",
        "\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + _x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSwY-e4cJ1ZK"
      },
      "source": [
        "### (4) Decoder\n",
        "<center><img src=\"https://yangoos57.github.io/static/9667d3a95db46e5f57616e57958c1061/3c492/decoder_1.png\" width=\"600\" height=\"500\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdHupxpKJ1ZK"
      },
      "source": [
        "decoder에 encoder의 output 외에 추가적으로 들어오는 input sentence\n",
        "- teacher forcing을 위함\n",
        "    - 모델 학스브 초창기에는 잘못된 token을 생성해낼 가능성이 큰데 이 잘못 예측된 token으로 다음 token도 예측하게 되면 제대로 된 학습이 진행되기 어려울 것\n",
        "    - 따라서, ground truth를 추가적으로 입력해주어 사용하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfQwCDf5J1ZP"
      },
      "source": [
        "<center><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/decoder.png\" height=\"200\" width=\"700\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506vmraQJ1ZP"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dec_voc_size, max_len, d_model, d_ff, n_heads, n_layers, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=dec_voc_size,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(d_model=d_model,\n",
        "                                                  d_ff=d_ff,\n",
        "                                                  n_heads=n_heads)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, context, no_look_mask, padding_mask):\n",
        "        x = self.emb(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, context, no_look_mask, padding_mask)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKoqst0J1ZP"
      },
      "source": [
        "## 4. Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwelXY5PJ1ZP"
      },
      "source": [
        "<center><img src=\"https://yangoos57.github.io/static/94719d02225cd94f0b1c085d1e741291/6af66/img11.png\" width=\"350\" height=\"500\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7XGCuxIJ1ZP"
      },
      "source": [
        "masking\n",
        "- (seq_len X seq_len) 크기의 mask matrix를 곱하는 방식으로 이루어짐\n",
        "- mask matrix에서 pad token에 해당하는 행과 열의 값은 0 (나머지는 1)\n",
        "- 반드시 Q와 K의 행렬곱 적용 이후, softmax 적용 전 수행되어야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo7DwtW_J1ZQ"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, pad_idx, no_look_idx, enc_voc_size, dec_voc_size, d_model, n_heads, max_len,\n",
        "                d_ff, n_layers, device):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.no_look_idx = no_look_idx\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               max_len=max_len,\n",
        "                               d_ff=d_ff,\n",
        "                               enc_voc_size=enc_voc_size,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "        self.decoder = Decoder(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               max_len=max_len,\n",
        "                               d_ff=d_ff,\n",
        "                               dec_voc_size=dec_voc_size,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "        self.generator = nn.Linear(d_model, dec_voc_size)\n",
        "\n",
        "    def make_pad_mask(self, src):\n",
        "        # src: [batch_size, seq_len]\n",
        "        pad_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2) # pad_idx와 일치하지 않는 토큰은 모두 0, 그 외는 모두 1인 mask 생성\n",
        "        # pad_mask: [batch_size, 1, 1, seq_len]\n",
        "        return pad_mask\n",
        "\n",
        "    def make_no_look_mask(self, trg):\n",
        "        # trg: [batch_size, seq_len]\n",
        "        pad_mask = (trg != self.no_look_idx).unsqueeze(1).unsqueeze(3)\n",
        "        # pad_mask: [batch_size, 1, 1, seq_len]\n",
        "        trg_len = trg.shape[1]\n",
        "        sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(device) # lower triangle\n",
        "        # sub_mask: [seq_len, trg_len]\n",
        "        no_look_mask = pad_mask & sub_mask\n",
        "        # no_look_mask: [batch_size, 1, trg_len, trg_len]\n",
        "        return no_look_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        padding_mask = self.make_pad_mask(src)\n",
        "        trg_mask = self.make_no_look_mask(trg)\n",
        "\n",
        "        context = self.encoder(src, padding_mask)\n",
        "        output = self.decoder(trg, context, trg_mask, padding_mask) # output: [batch_size, seq_len, d_model]\n",
        "\n",
        "        out = self.generator(output) # out: [batch_size, seq_len], 실제 token sequence를 원함, d_model을 len(vocab)으로 바꾸어주는 작업\n",
        "        out = F.log_softmax(out, dim=-1) # 마지막 dimension인 len에 대한 확률값을 구해야 하기 때문, 성능 향상 위해 log_softmax 사용\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, pad_idx, no_look_idx, encoder, decoder, dec_voc_size, d_model,device):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.no_look_idx = no_look_idx\n",
        "        self.device = device\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.generator = nn.Linear(d_model, dec_voc_size)\n",
        "\n",
        "    def make_pad_mask(self, src):\n",
        "        # src: [batch_size, seq_len]\n",
        "        pad_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2) # pad_idx와 일치하지 않는 토큰은 모두 0, 그 외는 모두 1인 mask 생성\n",
        "        # pad_mask: [batch_size, 1, 1, seq_len]\n",
        "        return pad_mask\n",
        "\n",
        "    def make_no_look_mask(self, trg):\n",
        "        # trg: [batch_size, seq_len]\n",
        "        pad_mask = (trg != self.no_look_idx).unsqueeze(1).unsqueeze(3)\n",
        "        # pad_mask: [batch_size, 1, 1, seq_len]\n",
        "        trg_len = trg.shape[1]\n",
        "        sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(device) # lower triangle\n",
        "        # sub_mask: [seq_len, trg_len]\n",
        "        no_look_mask = pad_mask & sub_mask\n",
        "        # no_look_mask: [batch_size, 1, trg_len, trg_len]\n",
        "        return no_look_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        padding_mask = self.make_pad_mask(src)\n",
        "        trg_mask = self.make_no_look_mask(trg)\n",
        "\n",
        "        context = self.encoder(src, padding_mask)\n",
        "        output = self.decoder(trg, context, trg_mask, padding_mask) # output: [batch_size, seq_len, d_model]\n",
        "\n",
        "        out = self.generator(output) # out: [batch_size, seq_len], 실제 token sequence를 원함, d_model을 len(vocab)으로 바꾸어주는 작업\n",
        "        out = F.log_softmax(out, dim=-1) # 마지막 dimension인 len에 대한 확률값을 구해야 하기 때문, 성능 향상 위해 log_softmax 사용\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "nxKZzkvaFEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dkqcmQzJ1ZQ"
      },
      "outputs": [],
      "source": [
        "# torch.tril(torch.ones(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습(Training)"
      ],
      "metadata": {
        "id": "L--zXSLzLuKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "MODEL_DIM = 128\n",
        "N_LAYERS = 2\n",
        "N_HEADS = 2\n",
        "FF_DIM = 258"
      ],
      "metadata": {
        "id": "V3FAnkE5LrXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, 100, MODEL_DIM, N_LAYERS, N_HEADS, FF_DIM, device)\n",
        "dec = Decoder(OUTPUT_DIM, 100, MODEL_DIM, FF_DIM, N_HEADS, N_LAYERS, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(SRC_PAD_IDX, TRG_PAD_IDX, enc,dec,OUTPUT_DIM, MODEL_DIM,device).to(device)"
      ],
      "metadata": {
        "id": "G8w5CZV6FacR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "# TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "# # 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "# enc = Encoder(INPUT_DIM, 100, MODEL_DIM, N_LAYERS, N_HEADS, FF_DIM, device)\n",
        "# dec = Decoder(OUTPUT_DIM, 100, MODEL_DIM, FF_DIM, N_HEADS, N_LAYERS, device)\n",
        "\n",
        "# # Transformer 객체 선언\n",
        "# model = Transformer(SRC_PAD_IDX, TRG_PAD_IDX, INPUT_DIM, OUTPUT_DIM, MODEL_DIM,\n",
        "#                     N_HEADS, 100, FF_DIM, N_LAYERS, device)"
      ],
      "metadata": {
        "id": "eNCk44CDMHHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 초기화\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTWVo1P0EVg9",
        "outputId": "b1f6be8d-4946-4da7-d523-9cdcfeda93b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 20,121,868 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtzOLrILEbzy",
        "outputId": "49866e4d-b6d4-4ccc-ba18-51700f08c555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (emb): TransformerEmbedding(\n",
              "      (tok_emb): TokenEmbedding(\n",
              "        (embedding): Embedding(7852, 128)\n",
              "      )\n",
              "      (pos_emb): PositionalEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-257): 258 x EncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_k): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_v): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_o): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (attention): ScaledDotProductAttention(\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (norm1): LayerNorm()\n",
              "        (ffn): PositionWiseFeedForward(\n",
              "          (linear1): Linear(in_features=128, out_features=2, bias=True)\n",
              "          (linear2): Linear(in_features=2, out_features=128, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (emb): TransformerEmbedding(\n",
              "      (tok_emb): TokenEmbedding(\n",
              "        (embedding): Embedding(5892, 128)\n",
              "      )\n",
              "      (pos_emb): PositionalEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x DecoderBlock(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_k): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_v): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_o): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (attention): ScaledDotProductAttention(\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (enc_dec_att): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_k): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_v): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (w_o): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (attention): ScaledDotProductAttention(\n",
              "            (softmax): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionWiseFeedForward(\n",
              "          (linear1): Linear(in_features=128, out_features=258, bias=True)\n",
              "          (linear2): Linear(in_features=258, out_features=128, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm3): LayerNorm()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (generator): Linear(in_features=128, out_features=5892, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "gj3-sNPtEdhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src.to(device)\n",
        "        trg = batch.trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스()는 제외\n",
        "        # 입력을 할 때는 부터 시작하도록 처리\n",
        "        output = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0()은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "5oVm_-4QEj86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스()는 제외\n",
        "            # 입력을 할 때는 부터 시작하도록 처리\n",
        "            output = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0()은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "RbBVG1EBEkiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습(training) 및 검증(validation) 진행\n",
        "## 학습 횟수(epoch): 10\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "Rpzf8TtMEnGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsGPFDEyEvhe",
        "outputId": "a5da910f-55d6-42aa-f734-aa728e9424ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 3m 56s\n",
            "\tTrain Loss: 3.776 | Train PPL: 43.642\n",
            "\tValidation Loss: 3.513 | Validation PPL: 33.561\n",
            "Epoch: 02 | Time: 3m 52s\n",
            "\tTrain Loss: 3.526 | Train PPL: 33.986\n",
            "\tValidation Loss: 3.369 | Validation PPL: 29.051\n",
            "Epoch: 03 | Time: 3m 52s\n",
            "\tTrain Loss: 3.369 | Train PPL: 29.061\n",
            "\tValidation Loss: 3.285 | Validation PPL: 26.722\n",
            "Epoch: 04 | Time: 3m 49s\n",
            "\tTrain Loss: 3.252 | Train PPL: 25.840\n",
            "\tValidation Loss: 3.218 | Validation PPL: 24.970\n",
            "Epoch: 05 | Time: 3m 47s\n",
            "\tTrain Loss: 3.159 | Train PPL: 23.555\n",
            "\tValidation Loss: 3.207 | Validation PPL: 24.700\n",
            "Epoch: 06 | Time: 3m 47s\n",
            "\tTrain Loss: 3.077 | Train PPL: 21.697\n",
            "\tValidation Loss: 3.219 | Validation PPL: 25.011\n",
            "Epoch: 07 | Time: 3m 58s\n",
            "\tTrain Loss: 3.002 | Train PPL: 20.131\n",
            "\tValidation Loss: 3.251 | Validation PPL: 25.811\n",
            "Epoch: 08 | Time: 4m 2s\n",
            "\tTrain Loss: 2.939 | Train PPL: 18.906\n",
            "\tValidation Loss: 3.286 | Validation PPL: 26.740\n",
            "Epoch: 09 | Time: 3m 51s\n",
            "\tTrain Loss: 2.882 | Train PPL: 17.841\n",
            "\tValidation Loss: 3.330 | Validation PPL: 27.952\n",
            "Epoch: 10 | Time: 3m 50s\n",
            "\tTrain Loss: 2.831 | Train PPL: 16.955\n",
            "\tValidation Loss: 3.419 | Validation PPL: 30.525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 학습된 모델 저장\n",
        "# from google.colab import files\n",
        "\n",
        "# files.download('transformer_german_to_english.pt')"
      ],
      "metadata": {
        "id": "rJsO82SMEx50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('transformer_german_to_english.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "metadata": {
        "id": "_5oulHamE1eA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b10f8cf-e6dc-4b39-f458-7e405562524c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.197 | Test PPL: 24.449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=100, logging=True):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에  토큰, 마지막에  토큰 붙이기\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    if logging:\n",
        "        print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    if logging:\n",
        "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # 소스 문장에 따른 마스크 생성\n",
        "    src_mask = model.make_pad_mask(src_tensor)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # 처음에는  토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        # 출력 문장에 따른 마스크 생성\n",
        "        trg_mask = model.make_no_look_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # 출력 문장에서 가장 마지막 단어만 사용\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # 를 만나는 순간 끝\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]\n"
      ],
      "metadata": {
        "id": "QjhSyVTmUS8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_dataset.examples[example_idx])['src']\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "\n",
        "translation = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "metadata": {
        "id": "fWLTQpxFUWAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a963b0-a0dd-4b4b-a171-6a104e33cb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n",
            "타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n",
            "전체 소스 토큰: ['', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '']\n",
            "소스 문장 인덱스: [2, 7, 363, 9, 133, 69, 623, 564, 18, 779, 199, 19, 87, 3, 2]\n",
            "모델 출력 결과: hat talking woman dogs person into boy into boy into boy into boy into jacket building building front building front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front front down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGc8Tv6WJ1ZQ"
      },
      "source": [
        "[References]\n",
        "- https://yangoos57.github.io/blog/DeepLearning/paper/Transformer/Transformer_From_Scratch/\n",
        "- https://jalammar.github.io/illustrated-transformer/\n",
        "- https://github.com/tunz/transformer-pytorch/blob/master/model/transformer.py\n",
        "- https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "- https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding\n",
        "- https://github.com/hyunwoongko/transformer/tree/master\n",
        "- https://code-angie.tistory.com/7\n",
        "- https://cpm0722.github.io/pytorch-implementation/transformer\n",
        "- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}